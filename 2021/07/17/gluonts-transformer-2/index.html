<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>GluonTS Transformer for Time Series (2) | Rickypp's blog</title><meta name="keywords" content="time series"><meta name="author" content="Rickypp"><meta name="copyright" content="Rickypp"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="GluonTS Transformer for Time Series part 2">
<meta property="og:type" content="article">
<meta property="og:title" content="GluonTS Transformer for Time Series (2)">
<meta property="og:url" content="https://r1ckypp.github.io/2021/07/17/gluonts-transformer-2/index.html">
<meta property="og:site_name" content="Rickypp&#39;s blog">
<meta property="og:description" content="GluonTS Transformer for Time Series part 2">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://r1ckypp.github.io/img/popcorn.png">
<meta property="article:published_time" content="2021-07-17T05:59:53.000Z">
<meta property="article:modified_time" content="2021-07-17T06:21:13.525Z">
<meta property="article:author" content="Rickypp">
<meta property="article:tag" content="time series">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://r1ckypp.github.io/img/popcorn.png"><link rel="shortcut icon" href="/img/strawberry.png"><link rel="canonical" href="https://r1ckypp.github.io/2021/07/17/gluonts-transformer-2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-07-17 14:21:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }})()</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/popcorn.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background: linear-gradient(20deg, #CCCCFF, #935696, #cc426e, #6E7D6)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Rickypp's blog</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">GluonTS Transformer for Time Series (2)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-17T05:59:53.000Z" title="发表于 2021-07-17 13:59:53">2021-07-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-17T06:21:13.525Z" title="更新于 2021-07-17 14:21:13">2021-07-17</time></span></div><div class="meta-secondline"> </div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Transformer-for-Time-Series-2"><a href="#Transformer-for-Time-Series-2" class="headerlink" title="Transformer for Time Series (2)"></a>Transformer for Time Series (2)</h1><p>细读一遍GluonTS Transformer的实现（原本代码的注释很详细了）</p>
<p>自地向上地读更细致，需要较长时间专注，只想了解思路的话建议自顶向下</p>
<p>这一篇看时序数据到底是怎么个训练以及推理过程</p>
<p><del>GluonTS对数据做了一些预处理，比如缩放、添加一些时序特征（lag等），这里不深入</del></p>
<p><del>主要看 TransformerTrainingNetwork 和 TransformerPredictionNetwork</del></p>
<p>好像不行，少了这部分，对encoder的输入过程会不清楚，还是简单看看吧</p>
<hr>
<h2 id="TransformerNetwork"><a href="#TransformerNetwork" class="headerlink" title="TransformerNetwork"></a>TransformerNetwork</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerNetwork</span>(<span class="params">mx.gluon.HybridBlock</span>):</span></span><br><span class="line"><span class="meta">    @validated()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder: TransformerEncoder,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder: TransformerDecoder,</span></span></span><br><span class="line"><span class="function"><span class="params">        history_length: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        context_length: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        prediction_length: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        distr_output: DistributionOutput,</span></span></span><br><span class="line"><span class="function"><span class="params">        cardinality: List[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">        embedding_dimension: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        lags_seq: List[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">        scaling: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        **kwargs,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">	</span><br><span class="line">        <span class="comment"># history_length = context_length + max(lags_seq)</span></span><br><span class="line">        <span class="comment"># history_length 理解成可用历史信息的最大长度，lags_seq是要用到的lag特征的index序列，详细内容在另一篇里细看</span></span><br><span class="line">        <span class="comment"># context_length 则是我们直接要使用的历史值</span></span><br><span class="line">        <span class="comment"># 两者的差别就是 history_length 范围内的值可能被我们选作特征(比如lag，具体会选哪些取决于我们lag index怎么设定)，而context_length直接代表输入的历史长度；前者理解成GluonTS针对时序特征工程做的一些额外工作，后者则是纯模型相关的</span></span><br><span class="line">        self.history_length = history_length</span><br><span class="line">        self.context_length = context_length</span><br><span class="line">        self.prediction_length = prediction_length</span><br><span class="line">        self.scaling = scaling</span><br><span class="line">        self.cardinality = cardinality</span><br><span class="line">        self.embedding_dimension = embedding_dimension</span><br><span class="line">        self.distr_output = distr_output</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(<span class="built_in">set</span>(lags_seq)) == <span class="built_in">len</span>(</span><br><span class="line">            lags_seq</span><br><span class="line">        ), <span class="string">&quot;no duplicated lags allowed!&quot;</span></span><br><span class="line">        lags_seq.sort()</span><br><span class="line"></span><br><span class="line">        self.lags_seq = lags_seq</span><br><span class="line"></span><br><span class="line">        self.target_shape = distr_output.event_shape</span><br><span class="line">        <span class="comment"># 这里及之后的target_shape默认为()，即默认没有该维度，prod(target_shape)=1</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.proj_dist_args = distr_output.get_args_proj()</span><br><span class="line">            self.encoder = encoder</span><br><span class="line">            self.decoder = decoder</span><br><span class="line">            self.embedder = FeatureEmbedder(</span><br><span class="line">                cardinalities=cardinality,</span><br><span class="line">                embedding_dims=[embedding_dimension <span class="keyword">for</span> _ <span class="keyword">in</span> cardinality],</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> scaling:</span><br><span class="line">                self.scaler = MeanScaler(keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.scaler = NOPScaler(keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lagged_subsequences</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">       	...</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Tensor:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 单独开一篇来看这些 time_feature</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_network_input</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        F,</span></span></span><br><span class="line"><span class="function"><span class="params">        feat_static_cat: Tensor,  <span class="comment"># (batch_size, num_features)</span></span></span></span><br><span class="line"><span class="function"><span class="params">        past_time_feat: Tensor,  <span class="comment"># (batch_size, history_length, num_features)</span></span></span></span><br><span class="line"><span class="function"><span class="params">        past_target: Tensor,  <span class="comment"># (batch_size, history_length, 1)</span></span></span></span><br><span class="line"><span class="function"><span class="params">        past_observed_values: Tensor,  <span class="comment"># (batch_size, history_length)</span></span></span></span><br><span class="line"><span class="function"><span class="params">        future_time_feat: Optional[</span></span></span><br><span class="line"><span class="function"><span class="params">            Tensor</span></span></span><br><span class="line"><span class="function"><span class="params">        ],  <span class="comment"># (batch_size, num_features, prediction_length)</span></span></span></span><br><span class="line"><span class="function"><span class="params">        future_target: Optional[Tensor],  <span class="comment"># (batch_size, prediction_length)</span></span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Tuple[Tensor, Tensor, Tensor]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Creates inputs for the transformer network.</span></span><br><span class="line"><span class="string">        All tensor arguments should have NTC layout.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 训练的时候是有future信息的，预测的时候没有</span></span><br><span class="line">        <span class="keyword">if</span> future_time_feat <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> future_target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            time_feat = past_time_feat.slice_axis(</span><br><span class="line">                axis=<span class="number">1</span>,</span><br><span class="line">                begin=self.history_length - self.context_length,</span><br><span class="line">                end=<span class="literal">None</span>,</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 把 context 部分拿出来作 time_feat</span></span><br><span class="line">            sequence = past_target</span><br><span class="line">            sequence_length = self.history_length</span><br><span class="line">            subsequences_length = self.context_length</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            time_feat = F.concat(</span><br><span class="line">                past_time_feat.slice_axis(</span><br><span class="line">                    axis=<span class="number">1</span>,</span><br><span class="line">                    begin=self.history_length - self.context_length,</span><br><span class="line">                    end=<span class="literal">None</span>,</span><br><span class="line">                ),</span><br><span class="line">                future_time_feat,</span><br><span class="line">                dim=<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">            sequence = F.concat(past_target, future_target, dim=<span class="number">1</span>)</span><br><span class="line">            sequence_length = self.history_length + self.prediction_length</span><br><span class="line">            subsequences_length = self.context_length + self.prediction_length</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch_size, sub_seq_len, *target_shape, num_lags)</span></span><br><span class="line">        lags = self.get_lagged_subsequences(</span><br><span class="line">            F=F,</span><br><span class="line">            sequence=sequence,</span><br><span class="line">            sequence_length=sequence_length,</span><br><span class="line">            indices=self.lags_seq,</span><br><span class="line">            subsequences_length=subsequences_length,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 获取lag特征，而具体哪些lag值默认是根据freq推理出来的，GluonTS依据经验来设定这些值，具体内容之后另开一篇讨论。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># scale is computed on the context length last units of the past target</span></span><br><span class="line">        <span class="comment"># scale shape is (batch_size, 1, *target_shape)</span></span><br><span class="line">        _, scale = self.scaler(</span><br><span class="line">            past_target.slice_axis(</span><br><span class="line">                axis=<span class="number">1</span>, begin=-self.context_length, end=<span class="literal">None</span></span><br><span class="line">            ),</span><br><span class="line">            past_observed_values.slice_axis(</span><br><span class="line">                axis=<span class="number">1</span>, begin=-self.context_length, end=<span class="literal">None</span></span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 对历史值作缩放</span></span><br><span class="line">        embedded_cat = self.embedder(feat_static_cat)</span><br><span class="line">        <span class="comment"># 对静态类别特征编码</span></span><br><span class="line">        <span class="comment"># in addition to embedding features, use the log scale as it can help prediction too</span></span><br><span class="line">        <span class="comment"># (batch_size, num_features + prod(target_shape))</span></span><br><span class="line">        static_feat = F.concat(</span><br><span class="line">            embedded_cat,</span><br><span class="line">            F.log(scale)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(self.target_shape) == <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span> F.log(scale.squeeze(axis=<span class="number">1</span>)),</span><br><span class="line">            dim=<span class="number">1</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 把 scale后历史值的log 值添加作 静态特征，说是有助于预测（神秘的特征工程~</span></span><br><span class="line"></span><br><span class="line">        repeated_static_feat = static_feat.expand_dims(axis=<span class="number">1</span>).repeat(</span><br><span class="line">            axis=<span class="number">1</span>, repeats=subsequences_length</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 静态特征 时间维度做拓展，方便后面拼接</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch_size, sub_seq_len, *target_shape, num_lags)</span></span><br><span class="line">        lags_scaled = F.broadcast_div(lags, scale.expand_dims(axis=<span class="number">-1</span>))</span><br><span class="line">        <span class="comment"># 对lag特征做缩放</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># from (batch_size, sub_seq_len, *target_shape, num_lags)</span></span><br><span class="line">        <span class="comment"># to (batch_size, sub_seq_len, prod(target_shape) * num_lags)</span></span><br><span class="line">        input_lags = F.reshape(</span><br><span class="line">            data=lags_scaled,</span><br><span class="line">            shape=(</span><br><span class="line">                <span class="number">-1</span>,</span><br><span class="line">                subsequences_length,</span><br><span class="line">                <span class="built_in">len</span>(self.lags_seq) * prod(self.target_shape),</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch_size, sub_seq_len, input_dim)</span></span><br><span class="line">        inputs = F.concat(input_lags, time_feat, repeated_static_feat, dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># mxnet计算时会自动推理input_dim，我们拼接好就不用管了</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inputs, scale, static_feat</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">upper_triangular_mask</span>(<span class="params">F, d</span>):</span></span><br><span class="line">        mask = F.zeros_like(F.eye(d))</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(d - <span class="number">1</span>):</span><br><span class="line">            mask = mask + F.eye(d, d, k + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> mask * LARGE_NEGATIVE_VALUE</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<hr>
<h2 id="TransformerTrainingNetwork"><a href="#TransformerTrainingNetwork" class="headerlink" title="TransformerTrainingNetwork"></a>TransformerTrainingNetwork</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerTrainingNetwork</span>(<span class="params">TransformerNetwork</span>):</span></span><br><span class="line">    <span class="comment"># noinspection PyMethodOverriding,PyPep8Naming</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        F,</span></span></span><br><span class="line"><span class="function"><span class="params">        feat_static_cat: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        past_time_feat: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        past_target: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        past_observed_values: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        future_time_feat: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        future_target: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Computes the loss for training Transformer, all inputs tensors representing time series have NTC layout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        F</span></span><br><span class="line"><span class="string">        feat_static_cat : (batch_size, num_features)</span></span><br><span class="line"><span class="string">        past_time_feat : (batch_size, history_length, num_features)</span></span><br><span class="line"><span class="string">        past_target : (batch_size, history_length, *target_shape)</span></span><br><span class="line"><span class="string">        past_observed_values : (batch_size, history_length, *target_shape, seq_len)</span></span><br><span class="line"><span class="string">        future_time_feat : (batch_size, prediction_length, num_features)</span></span><br><span class="line"><span class="string">        future_target : (batch_size, prediction_length, *target_shape)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        Loss with shape (batch_size, context + prediction_length, 1)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create the inputs for the encoder</span></span><br><span class="line">        inputs, scale, _ = self.create_network_input(</span><br><span class="line">            F=F,</span><br><span class="line">            feat_static_cat=feat_static_cat,</span><br><span class="line">            past_time_feat=past_time_feat,</span><br><span class="line">            past_target=past_target,</span><br><span class="line">            past_observed_values=past_observed_values,</span><br><span class="line">            future_time_feat=future_time_feat,</span><br><span class="line">            future_target=future_target,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        enc_input = F.slice_axis(</span><br><span class="line">            inputs, axis=<span class="number">1</span>, begin=<span class="number">0</span>, end=self.context_length</span><br><span class="line">        )</span><br><span class="line">        dec_input = F.slice_axis(</span><br><span class="line">            inputs, axis=<span class="number">1</span>, begin=self.context_length, end=<span class="literal">None</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass through encoder</span></span><br><span class="line">        enc_out = self.encoder(enc_input)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input to decoder</span></span><br><span class="line">        dec_output = self.decoder(</span><br><span class="line">            dec_input,</span><br><span class="line">            enc_out,</span><br><span class="line">            self.upper_triangular_mask(F, self.prediction_length),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute loss</span></span><br><span class="line">        distr_args = self.proj_dist_args(dec_output)</span><br><span class="line">        distr = self.distr_output.distribution(distr_args, scale=scale)</span><br><span class="line">        loss = distr.loss(future_target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br></pre></td></tr></table></figure>
<p>几个重要点：</p>
<ul>
<li>encoder的输入是历史值，decoder的输入是label（要预测的值）</li>
<li>GluonTS关注概率预测，所以这里最后模型输出的是分布的参数，再从分布采样来计算最终预测值以及loss（更多细节可以看Estimator、gluonts.mx.distribution等的实现)，TransformerEstimator中默认 <em>distr_output</em>: DistributionOutput <em>=</em> StudentTOutput()，其event_shape = ()，没错，一个空的tuple</li>
</ul>
<hr>
<h2 id="TransformerPredictionNetwork"><a href="#TransformerPredictionNetwork" class="headerlink" title="TransformerPredictionNetwork"></a>TransformerPredictionNetwork</h2><p>预测的时候decoder的用法不一样了！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerPredictionNetwork</span>(<span class="params">TransformerNetwork</span>):</span></span><br><span class="line"><span class="meta">    @validated()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_parallel_samples: <span class="built_in">int</span> = <span class="number">100</span>, **kwargs</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.num_parallel_samples = num_parallel_samples</span><br><span class="line"></span><br><span class="line">        <span class="comment"># for decoding the lags are shifted by one,</span></span><br><span class="line">        <span class="comment"># at the first time-step of the decoder a lag of one corresponds to the last target value</span></span><br><span class="line">        self.shifted_lags = [l - <span class="number">1</span> <span class="keyword">for</span> l <span class="keyword">in</span> self.lags_seq]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sampling_decoder</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        F,</span></span></span><br><span class="line"><span class="function"><span class="params">        static_feat: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        past_target: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        time_feat: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        scale: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">        enc_out: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Computes sample paths by unrolling the LSTM starting with a initial input and state.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        static_feat : Tensor</span></span><br><span class="line"><span class="string">            static features. Shape: (batch_size, num_static_features).</span></span><br><span class="line"><span class="string">        past_target : Tensor</span></span><br><span class="line"><span class="string">            target history. Shape: (batch_size, history_length, 1).</span></span><br><span class="line"><span class="string">        time_feat : Tensor</span></span><br><span class="line"><span class="string">            time features. Shape: (batch_size, prediction_length, num_time_features).</span></span><br><span class="line"><span class="string">        scale : Tensor</span></span><br><span class="line"><span class="string">            tensor containing the scale of each element in the batch. Shape: (batch_size, ).</span></span><br><span class="line"><span class="string">        enc_out: Tensor</span></span><br><span class="line"><span class="string">            output of the encoder. Shape: (batch_size, num_cells)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        --------</span></span><br><span class="line"><span class="string">        sample_paths : Tensor</span></span><br><span class="line"><span class="string">            a tensor containing sampled paths. Shape: (batch_size, num_sample_paths, prediction_length).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># blows-up the dimension of each tensor to batch_size * self.num_parallel_samples for increasing parallelism</span></span><br><span class="line">        repeated_past_target = past_target.repeat(</span><br><span class="line">            repeats=self.num_parallel_samples, axis=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        future_samples = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># for each future time-units we draw new samples for this time-unit and update the state</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(self.prediction_length):</span><br><span class="line">            lags = self.get_lagged_subsequences(</span><br><span class="line">                F=F,</span><br><span class="line">                sequence=repeated_past_target,</span><br><span class="line">                sequence_length=self.history_length + k,</span><br><span class="line">                indices=self.shifted_lags,</span><br><span class="line">                subsequences_length=<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 获取lag特征，而具体哪些lag值默认是根据freq推理出来的，GluonTS依据经验来设定这些值，具体内容之后另开一篇讨论。</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch_size * num_samples, 1, *target_shape, num_lags)</span></span><br><span class="line">            lags_scaled = F.broadcast_div(</span><br><span class="line">                lags, repeated_scale.expand_dims(axis=<span class="number">-1</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 把lag值缩放一下</span></span><br><span class="line">            <span class="comment"># 这里及之后的shape里的 1 代表的是我们每次单步预测</span></span><br><span class="line">            <span class="comment"># 这里及之后的target_shape默认为()，即默认没有该维度，prod(target_shape)=1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># from (batch_size * num_samples, 1, *target_shape, num_lags)</span></span><br><span class="line">            <span class="comment"># to (batch_size * num_samples, 1, prod(target_shape) * num_lags)</span></span><br><span class="line">            input_lags = F.reshape(</span><br><span class="line">                data=lags_scaled,</span><br><span class="line">                shape=(<span class="number">-1</span>, <span class="number">1</span>, prod(self.target_shape) * <span class="built_in">len</span>(self.lags_seq)),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch_size * num_samples, 1, prod(target_shape) * num_lags + num_time_features + num_static_features)</span></span><br><span class="line">            dec_input = F.concat(</span><br><span class="line">                input_lags,</span><br><span class="line">                repeated_time_feat.slice_axis(axis=<span class="number">1</span>, begin=k, end=k + <span class="number">1</span>),</span><br><span class="line">                repeated_static_feat,</span><br><span class="line">                dim=<span class="number">-1</span>,</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 特征维度拼接到一起，这样我们就有了 lag特征+动态特征+静态特征</span></span><br><span class="line"></span><br><span class="line">            dec_output = self.decoder(dec_input, repeated_enc_out, <span class="literal">None</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">            distr_args = self.proj_dist_args(dec_output)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute likelihood of target given the predicted parameters</span></span><br><span class="line">            distr = self.distr_output.distribution(</span><br><span class="line">                distr_args, scale=repeated_scale</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch_size * num_samples, 1, *target_shape)</span></span><br><span class="line">            new_samples = distr.sample()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (batch_size * num_samples, seq_len, *target_shape)</span></span><br><span class="line">            repeated_past_target = F.concat(</span><br><span class="line">                repeated_past_target, new_samples, dim=<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 历史值 跟 新预测的值 时间维度拼接到一块儿；因此下一次循环sequence_length增加了1</span></span><br><span class="line">            future_samples.append(new_samples)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reset cache of the decoder</span></span><br><span class="line">        self.decoder.cache_reset()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch_size * num_samples, prediction_length, *target_shape)</span></span><br><span class="line">        samples = F.concat(*future_samples, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 把所有预测的值拼起来</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch_size, num_samples, *target_shape, prediction_length)</span></span><br><span class="line">        <span class="keyword">return</span> samples.reshape(</span><br><span class="line">            shape=(</span><br><span class="line">                (<span class="number">-1</span>, self.num_parallel_samples)</span><br><span class="line">                + self.target_shape</span><br><span class="line">                + (self.prediction_length,)</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    	<span class="comment"># 这里shape是tuple拼接哦，第一次看没看懂~</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># noinspection PyMethodOverriding,PyPep8Naming</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        ...</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Tensor:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create the inputs for the encoder</span></span><br><span class="line">        inputs, scale, static_feat = self.create_network_input(</span><br><span class="line">            ...</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass through encoder</span></span><br><span class="line">        enc_out = self.encoder(inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.sampling_decoder(</span><br><span class="line">            ...</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<blockquote>
<p><em>Lags are target values at the same <code>season</code> (+/- delta) but in the previous cycle.</em></p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>Transformer的decoder在训练时和预测时方法是不同的<ul>
<li>训练时，一趟即可产出所有预测值，然后计算Loss</li>
<li>预测时，采用rolling predict的方式，多次单步预测<ul>
<li>回想上篇留下的坑，KV在预测时、Decoder里、self-attention上进行了一个神秘的cache操作；我们decoder在预测时多次 forward，每次 1 step的输入，该输入被转换成QKV后，Q时间上总是1 step，但KV的时间维度是不断增长的</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>训练和预测的差异更深层次的原因是什么呢</p>
<h2 id="不清楚的地方"><a href="#不清楚的地方" class="headerlink" title="不清楚的地方"></a>不清楚的地方</h2><h3 id="神秘的DistributionOutput"><a href="#神秘的DistributionOutput" class="headerlink" title="神秘的DistributionOutput"></a>神秘的DistributionOutput</h3><p>从TransformerEstimator可以知道默认输出分布为StudentTOutput()，一般没人会改吧，什么情况下需要改呢？？直接影响到target_shape，输出会变成什么呢？怎么理解？？</p>
<h3 id="神秘的时序数据特征工程"><a href="#神秘的时序数据特征工程" class="headerlink" title="神秘的时序数据特征工程"></a>神秘的时序数据特征工程</h3><p>好吧，也没那么神秘，可以总结一波，留坑</p>
<h3 id="神秘的-typo"><a href="#神秘的-typo" class="headerlink" title="神秘的 typo"></a>神秘的 typo</h3><p>发现源码注释里有些 typo，要不要提交PR帮他们改改捏</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Rickypp</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://r1ckypp.github.io/2021/07/17/gluonts-transformer-2/">https://r1ckypp.github.io/2021/07/17/gluonts-transformer-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://r1ckypp.github.io" target="_blank">Rickypp's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/time-series/">time series</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/07/17/gluonts-transformer-1/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">GluonTS Transformer for Time Series (1)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/11/05/time-series-material/" title="Time Series Material"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-05</div><div class="title">Time Series Material</div></div></a></div><div><a href="/2020/12/07/time-series-forecast-covariate/" title="Time Series Forecast -- Covariate"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-07</div><div class="title">Time Series Forecast -- Covariate</div></div></a></div><div><a href="/2020/12/09/transformer-review/" title="Transformer for Time Series review"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Transformer for Time Series review</div></div></a></div><div><a href="/2021/07/17/gluonts-transformer-1/" title="GluonTS Transformer for Time Series (1)"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-17</div><div class="title">GluonTS Transformer for Time Series (1)</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">还在装修~</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-for-Time-Series-2"><span class="toc-number">1.</span> <span class="toc-text">Transformer for Time Series (2)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TransformerNetwork"><span class="toc-number">1.1.</span> <span class="toc-text">TransformerNetwork</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TransformerTrainingNetwork"><span class="toc-number">1.2.</span> <span class="toc-text">TransformerTrainingNetwork</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TransformerPredictionNetwork"><span class="toc-number">1.3.</span> <span class="toc-text">TransformerPredictionNetwork</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E6%B8%85%E6%A5%9A%E7%9A%84%E5%9C%B0%E6%96%B9"><span class="toc-number">1.5.</span> <span class="toc-text">不清楚的地方</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%A7%98%E7%9A%84DistributionOutput"><span class="toc-number">1.5.1.</span> <span class="toc-text">神秘的DistributionOutput</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%A7%98%E7%9A%84%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">1.5.2.</span> <span class="toc-text">神秘的时序数据特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%A7%98%E7%9A%84-typo"><span class="toc-number">1.5.3.</span> <span class="toc-text">神秘的 typo</span></a></li></ol></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/07/17/gluonts-transformer-2/" title="GluonTS Transformer for Time Series (2)">GluonTS Transformer for Time Series (2)</a><time datetime="2021-07-17T05:59:53.000Z" title="发表于 2021-07-17 13:59:53">2021-07-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/07/17/gluonts-transformer-1/" title="GluonTS Transformer for Time Series (1)">GluonTS Transformer for Time Series (1)</a><time datetime="2021-07-17T05:58:53.000Z" title="发表于 2021-07-17 13:58:53">2021-07-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/06/10/trivial-0/" title="trivial-0">trivial-0</a><time datetime="2021-06-10T02:52:52.000Z" title="发表于 2021-06-10 10:52:52">2021-06-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/06/09/In-database-ML-survey/" title="In-database ML survey">In-database ML survey</a><time datetime="2021-06-09T09:21:12.000Z" title="发表于 2021-06-09 17:21:12">2021-06-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/04/02/aiops2021-summary/" title="记第一次参加算法比赛">记第一次参加算法比赛</a><time datetime="2021-04-02T03:18:52.000Z" title="发表于 2021-04-02 11:18:52">2021-04-02</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="footer_custom_text">很懒的啦~</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"></div></div></body></html>